{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "./datasets/ss.csv\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "Entropy of TPCDS([Column(ss_sold_date_sk, distribution_size=94), Column(ss_store_sk, distribution_size=44), Column(ss_sales_price, distribution_size=99)]): 6.6439 bits\r\n",
      "<class 'pandas.core.frame.DataFrame'>\r\n",
      "RangeIndex: 100 entries, 0 to 99\r\n",
      "Data columns (total 3 columns):\r\n",
      " #   Column           Non-Null Count  Dtype  \r\n",
      "---  ------           --------------  -----  \r\n",
      " 0   ss_sold_date_sk  97 non-null     float64\r\n",
      " 1   ss_store_sk      98 non-null     float64\r\n",
      " 2   ss_sales_price   98 non-null     float64\r\n",
      "dtypes: float64(3)\r\n",
      "memory usage: 2.5 KB\r\n",
      "None\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99]\r\n",
      "encoded_bins (input) [7, 6, 7]\r\n",
      "Number of model parameters: 132333 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=20, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=237, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Applying InitWeight()\r\n",
      "Discretizing table... done, took 0.0s\r\n",
      "Epoch 0 Iter 0, train entropy gap 12.0770 bits (loss 18.721, data 6.644) 0.01000 lr\r\n",
      "epoch 0 train loss 12.9763 nats / 18.7209 bits\r\n",
      "time since start: 0.0 secs\r\n",
      "Epoch 1 Iter 0, train entropy gap 11.3176 bits (loss 17.961, data 6.644) 0.01000 lr\r\n",
      "epoch 1 train loss 12.4499 nats / 17.9615 bits\r\n",
      "time since start: 0.0 secs\r\n",
      "Epoch 2 Iter 0, train entropy gap 10.4997 bits (loss 17.144, data 6.644) 0.01000 lr\r\n",
      "epoch 2 train loss 11.8830 nats / 17.1435 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 3 Iter 0, train entropy gap 9.4542 bits (loss 16.098, data 6.644) 0.01000 lr\r\n",
      "epoch 3 train loss 11.1583 nats / 16.0980 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 4 Iter 0, train entropy gap 8.0640 bits (loss 14.708, data 6.644) 0.01000 lr\r\n",
      "epoch 4 train loss 10.1947 nats / 14.7079 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 5 Iter 0, train entropy gap 6.5941 bits (loss 13.238, data 6.644) 0.01000 lr\r\n",
      "epoch 5 train loss 9.1759 nats / 13.2380 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 6 Iter 0, train entropy gap 5.7175 bits (loss 12.361, data 6.644) 0.01000 lr\r\n",
      "epoch 6 train loss 8.5682 nats / 12.3613 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 7 Iter 0, train entropy gap 4.8018 bits (loss 11.446, data 6.644) 0.01000 lr\r\n",
      "epoch 7 train loss 7.9335 nats / 11.4456 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 8 Iter 0, train entropy gap 4.1700 bits (loss 10.814, data 6.644) 0.01000 lr\r\n",
      "epoch 8 train loss 7.4956 nats / 10.8139 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 9 Iter 0, train entropy gap 3.5026 bits (loss 10.146, data 6.644) 0.01000 lr\r\n",
      "epoch 9 train loss 7.0330 nats / 10.1465 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 10 Iter 0, train entropy gap 3.3161 bits (loss 9.960, data 6.644) 0.01000 lr\r\n",
      "epoch 10 train loss 6.9037 nats / 9.9599 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 11 Iter 0, train entropy gap 2.8844 bits (loss 9.528, data 6.644) 0.01000 lr\r\n",
      "epoch 11 train loss 6.6045 nats / 9.5282 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 12 Iter 0, train entropy gap 2.6466 bits (loss 9.290, data 6.644) 0.01000 lr\r\n",
      "epoch 12 train loss 6.4397 nats / 9.2905 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 13 Iter 0, train entropy gap 2.4195 bits (loss 9.063, data 6.644) 0.01000 lr\r\n",
      "epoch 13 train loss 6.2822 nats / 9.0634 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 14 Iter 0, train entropy gap 2.1469 bits (loss 8.791, data 6.644) 0.01000 lr\r\n",
      "epoch 14 train loss 6.0933 nats / 8.7908 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 15 Iter 0, train entropy gap 1.9531 bits (loss 8.597, data 6.644) 0.01000 lr\r\n",
      "epoch 15 train loss 5.9590 nats / 8.5970 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 16 Iter 0, train entropy gap 1.7966 bits (loss 8.440, data 6.644) 0.01000 lr\r\n",
      "epoch 16 train loss 5.8505 nats / 8.4405 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 17 Iter 0, train entropy gap 1.8552 bits (loss 8.499, data 6.644) 0.01000 lr\r\n",
      "epoch 17 train loss 5.8911 nats / 8.4990 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 18 Iter 0, train entropy gap 1.5238 bits (loss 8.168, data 6.644) 0.01000 lr\r\n",
      "epoch 18 train loss 5.6614 nats / 8.1677 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 19 Iter 0, train entropy gap 1.3876 bits (loss 8.031, data 6.644) 0.01000 lr\r\n",
      "epoch 19 train loss 5.5670 nats / 8.0314 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 20 Iter 0, train entropy gap 1.2083 bits (loss 7.852, data 6.644) 0.01000 lr\r\n",
      "epoch 20 train loss 5.4427 nats / 7.8522 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 21 Iter 0, train entropy gap 0.9512 bits (loss 7.595, data 6.644) 0.01000 lr\r\n",
      "epoch 21 train loss 5.2645 nats / 7.5950 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 22 Iter 0, train entropy gap 0.9577 bits (loss 7.602, data 6.644) 0.01000 lr\r\n",
      "epoch 22 train loss 5.2690 nats / 7.6016 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 23 Iter 0, train entropy gap 0.6040 bits (loss 7.248, data 6.644) 0.01000 lr\r\n",
      "epoch 23 train loss 5.0238 nats / 7.2478 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 24 Iter 0, train entropy gap 0.6001 bits (loss 7.244, data 6.644) 0.01000 lr\r\n",
      "epoch 24 train loss 5.0211 nats / 7.2440 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 25 Iter 0, train entropy gap 0.3905 bits (loss 7.034, data 6.644) 0.01000 lr\r\n",
      "epoch 25 train loss 4.8759 nats / 7.0344 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 26 Iter 0, train entropy gap 0.3538 bits (loss 6.998, data 6.644) 0.01000 lr\r\n",
      "epoch 26 train loss 4.8504 nats / 6.9977 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 27 Iter 0, train entropy gap 0.3005 bits (loss 6.944, data 6.644) 0.01000 lr\r\n",
      "epoch 27 train loss 4.8135 nats / 6.9444 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 28 Iter 0, train entropy gap 0.1888 bits (loss 6.833, data 6.644) 0.01000 lr\r\n",
      "epoch 28 train loss 4.7360 nats / 6.8326 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 29 Iter 0, train entropy gap 0.1769 bits (loss 6.821, data 6.644) 0.01000 lr\r\n",
      "epoch 29 train loss 4.7278 nats / 6.8208 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 30 Iter 0, train entropy gap 0.1173 bits (loss 6.761, data 6.644) 0.01000 lr\r\n",
      "epoch 30 train loss 4.6865 nats / 6.7612 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 31 Iter 0, train entropy gap 0.1011 bits (loss 6.745, data 6.644) 0.01000 lr\r\n",
      "epoch 31 train loss 4.6752 nats / 6.7449 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 32 Iter 0, train entropy gap 0.1008 bits (loss 6.745, data 6.644) 0.01000 lr\r\n",
      "epoch 32 train loss 4.6751 nats / 6.7447 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 33 Iter 0, train entropy gap 0.0959 bits (loss 6.740, data 6.644) 0.01000 lr\r\n",
      "epoch 33 train loss 4.6716 nats / 6.7397 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 34 Iter 0, train entropy gap 0.0986 bits (loss 6.742, data 6.644) 0.01000 lr\r\n",
      "epoch 34 train loss 4.6735 nats / 6.7424 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 35 Iter 0, train entropy gap 0.0703 bits (loss 6.714, data 6.644) 0.01000 lr\r\n",
      "epoch 35 train loss 4.6539 nats / 6.7142 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 36 Iter 0, train entropy gap 0.0401 bits (loss 6.684, data 6.644) 0.01000 lr\r\n",
      "epoch 36 train loss 4.6329 nats / 6.6839 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 37 Iter 0, train entropy gap 0.1723 bits (loss 6.816, data 6.644) 0.01000 lr\r\n",
      "epoch 37 train loss 4.7246 nats / 6.8162 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 38 Iter 0, train entropy gap 0.0491 bits (loss 6.693, data 6.644) 0.01000 lr\r\n",
      "epoch 38 train loss 4.6392 nats / 6.6929 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 39 Iter 0, train entropy gap 0.3476 bits (loss 6.991, data 6.644) 0.01000 lr\r\n",
      "epoch 39 train loss 4.8461 nats / 6.9914 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 40 Iter 0, train entropy gap 0.1189 bits (loss 6.763, data 6.644) 0.01000 lr\r\n",
      "epoch 40 train loss 4.6876 nats / 6.7628 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 41 Iter 0, train entropy gap 0.0431 bits (loss 6.687, data 6.644) 0.01000 lr\r\n",
      "epoch 41 train loss 4.6351 nats / 6.6870 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 42 Iter 0, train entropy gap 0.3066 bits (loss 6.950, data 6.644) 0.01000 lr\r\n",
      "epoch 42 train loss 4.8177 nats / 6.9504 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 43 Iter 0, train entropy gap 0.2758 bits (loss 6.920, data 6.644) 0.01000 lr\r\n",
      "epoch 43 train loss 4.7963 nats / 6.9196 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 44 Iter 0, train entropy gap 0.0320 bits (loss 6.676, data 6.644) 0.01000 lr\r\n",
      "epoch 44 train loss 4.6273 nats / 6.6759 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 45 Iter 0, train entropy gap 0.0182 bits (loss 6.662, data 6.644) 0.01000 lr\r\n",
      "epoch 45 train loss 4.6178 nats / 6.6621 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 46 Iter 0, train entropy gap 0.0230 bits (loss 6.667, data 6.644) 0.01000 lr\r\n",
      "epoch 46 train loss 4.6211 nats / 6.6668 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 47 Iter 0, train entropy gap 0.0162 bits (loss 6.660, data 6.644) 0.01000 lr\r\n",
      "epoch 47 train loss 4.6164 nats / 6.6601 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 48 Iter 0, train entropy gap 0.0208 bits (loss 6.665, data 6.644) 0.01000 lr\r\n",
      "epoch 48 train loss 4.6196 nats / 6.6647 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 49 Iter 0, train entropy gap 0.0417 bits (loss 6.686, data 6.644) 0.01000 lr\r\n",
      "epoch 49 train loss 4.6341 nats / 6.6856 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 50 Iter 0, train entropy gap 0.0321 bits (loss 6.676, data 6.644) 0.01000 lr\r\n",
      "epoch 50 train loss 4.6274 nats / 6.6759 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 51 Iter 0, train entropy gap 0.0164 bits (loss 6.660, data 6.644) 0.01000 lr\r\n",
      "epoch 51 train loss 4.6166 nats / 6.6603 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 52 Iter 0, train entropy gap 0.0100 bits (loss 6.654, data 6.644) 0.01000 lr\r\n",
      "epoch 52 train loss 4.6121 nats / 6.6538 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 53 Iter 0, train entropy gap 0.0123 bits (loss 6.656, data 6.644) 0.01000 lr\r\n",
      "epoch 53 train loss 4.6137 nats / 6.6562 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 54 Iter 0, train entropy gap 0.0085 bits (loss 6.652, data 6.644) 0.01000 lr\r\n",
      "epoch 54 train loss 4.6110 nats / 6.6523 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 55 Iter 0, train entropy gap 0.0105 bits (loss 6.654, data 6.644) 0.01000 lr\r\n",
      "epoch 55 train loss 4.6125 nats / 6.6544 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 56 Iter 0, train entropy gap 0.0128 bits (loss 6.657, data 6.644) 0.01000 lr\r\n",
      "epoch 56 train loss 4.6141 nats / 6.6567 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 57 Iter 0, train entropy gap 0.0104 bits (loss 6.654, data 6.644) 0.01000 lr\r\n",
      "epoch 57 train loss 4.6123 nats / 6.6542 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 58 Iter 0, train entropy gap 0.0087 bits (loss 6.653, data 6.644) 0.01000 lr\r\n",
      "epoch 58 train loss 4.6112 nats / 6.6526 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 59 Iter 0, train entropy gap 0.0059 bits (loss 6.650, data 6.644) 0.01000 lr\r\n",
      "epoch 59 train loss 4.6093 nats / 6.6498 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 60 Iter 0, train entropy gap 0.0055 bits (loss 6.649, data 6.644) 0.01000 lr\r\n",
      "epoch 60 train loss 4.6090 nats / 6.6494 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 61 Iter 0, train entropy gap 0.0067 bits (loss 6.651, data 6.644) 0.01000 lr\r\n",
      "epoch 61 train loss 4.6098 nats / 6.6506 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 62 Iter 0, train entropy gap 0.0058 bits (loss 6.650, data 6.644) 0.01000 lr\r\n",
      "epoch 62 train loss 4.6092 nats / 6.6497 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 63 Iter 0, train entropy gap 0.0048 bits (loss 6.649, data 6.644) 0.01000 lr\r\n",
      "epoch 63 train loss 4.6085 nats / 6.6487 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 64 Iter 0, train entropy gap 0.0040 bits (loss 6.648, data 6.644) 0.01000 lr\r\n",
      "epoch 64 train loss 4.6079 nats / 6.6478 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 65 Iter 0, train entropy gap 0.0039 bits (loss 6.648, data 6.644) 0.01000 lr\r\n",
      "epoch 65 train loss 4.6079 nats / 6.6477 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 66 Iter 0, train entropy gap 0.0048 bits (loss 6.649, data 6.644) 0.01000 lr\r\n",
      "epoch 66 train loss 4.6085 nats / 6.6486 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 67 Iter 0, train entropy gap 0.0036 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 67 train loss 4.6076 nats / 6.6474 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 68 Iter 0, train entropy gap 0.0023 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 68 train loss 4.6067 nats / 6.6461 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 69 Iter 0, train entropy gap 0.0030 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 69 train loss 4.6072 nats / 6.6468 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 70 Iter 0, train entropy gap 0.0030 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 70 train loss 4.6072 nats / 6.6468 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 71 Iter 0, train entropy gap 0.0030 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 71 train loss 4.6072 nats / 6.6468 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 72 Iter 0, train entropy gap 0.0025 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 72 train loss 4.6069 nats / 6.6464 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 73 Iter 0, train entropy gap 0.0019 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 73 train loss 4.6065 nats / 6.6457 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 74 Iter 0, train entropy gap 0.0019 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 74 train loss 4.6065 nats / 6.6458 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 75 Iter 0, train entropy gap 0.0018 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 75 train loss 4.6064 nats / 6.6457 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 76 Iter 0, train entropy gap 0.0020 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 76 train loss 4.6066 nats / 6.6459 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 77 Iter 0, train entropy gap 0.0022 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 77 train loss 4.6067 nats / 6.6461 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 78 Iter 0, train entropy gap 0.0019 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 78 train loss 4.6065 nats / 6.6458 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 79 Iter 0, train entropy gap 0.0018 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 79 train loss 4.6064 nats / 6.6456 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 80 Iter 0, train entropy gap 0.0017 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 80 train loss 4.6064 nats / 6.6456 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 81 Iter 0, train entropy gap 0.0014 bits (loss 6.645, data 6.644) 0.01000 lr\r\n",
      "epoch 81 train loss 4.6061 nats / 6.6452 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 82 Iter 0, train entropy gap 0.0014 bits (loss 6.645, data 6.644) 0.01000 lr\r\n",
      "epoch 82 train loss 4.6062 nats / 6.6453 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 83 Iter 0, train entropy gap 0.0017 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 83 train loss 4.6064 nats / 6.6456 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 84 Iter 0, train entropy gap 0.0021 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 84 train loss 4.6066 nats / 6.6459 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 85 Iter 0, train entropy gap 0.0032 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 85 train loss 4.6074 nats / 6.6471 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 86 Iter 0, train entropy gap 0.0052 bits (loss 6.649, data 6.644) 0.01000 lr\r\n",
      "epoch 86 train loss 4.6088 nats / 6.6490 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 87 Iter 0, train entropy gap 0.0070 bits (loss 6.651, data 6.644) 0.01000 lr\r\n",
      "epoch 87 train loss 4.6100 nats / 6.6509 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 88 Iter 0, train entropy gap 0.0078 bits (loss 6.652, data 6.644) 0.01000 lr\r\n",
      "epoch 88 train loss 4.6106 nats / 6.6516 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 89 Iter 0, train entropy gap 0.0066 bits (loss 6.650, data 6.644) 0.01000 lr\r\n",
      "epoch 89 train loss 4.6098 nats / 6.6505 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 90 Iter 0, train entropy gap 0.0050 bits (loss 6.649, data 6.644) 0.01000 lr\r\n",
      "epoch 90 train loss 4.6087 nats / 6.6489 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 91 Iter 0, train entropy gap 0.0032 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 91 train loss 4.6074 nats / 6.6471 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 92 Iter 0, train entropy gap 0.0017 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 92 train loss 4.6063 nats / 6.6455 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 93 Iter 0, train entropy gap 0.0009 bits (loss 6.645, data 6.644) 0.01000 lr\r\n",
      "epoch 93 train loss 4.6058 nats / 6.6447 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 94 Iter 0, train entropy gap 0.0009 bits (loss 6.645, data 6.644) 0.01000 lr\r\n",
      "epoch 94 train loss 4.6058 nats / 6.6448 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 95 Iter 0, train entropy gap 0.0016 bits (loss 6.645, data 6.644) 0.01000 lr\r\n",
      "epoch 95 train loss 4.6063 nats / 6.6455 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 96 Iter 0, train entropy gap 0.0026 bits (loss 6.646, data 6.644) 0.01000 lr\r\n",
      "epoch 96 train loss 4.6069 nats / 6.6464 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 97 Iter 0, train entropy gap 0.0036 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 97 train loss 4.6076 nats / 6.6474 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 98 Iter 0, train entropy gap 0.0039 bits (loss 6.648, data 6.644) 0.01000 lr\r\n",
      "epoch 98 train loss 4.6079 nats / 6.6477 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 99 Iter 0, train entropy gap 0.0034 bits (loss 6.647, data 6.644) 0.01000 lr\r\n",
      "epoch 99 train loss 4.6075 nats / 6.6472 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Training done; evaluating likelihood on full data:\r\n",
      "Epoch None Iter 0, test loss 4.6069 nats / 6.6464 bits\r\n",
      "Saved to:\r\n",
      "models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --epochs=100 --residual --dataset=ss.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "ckpts ['./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "./datasets/ss.csv\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99]\r\n",
      "encoded_bins (input) [7, 6, 7]\r\n",
      "Number of model parameters: 132333 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=20, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=237, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt 0.0019999999999997797 0\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "where_col [Column(ss_sold_date_sk, distribution_size=94)]\r\n",
      "1.9453609023844973 0.08007598891854287 2.0443894766762063\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob='ss*.pt' --dataset=ss.csv --residual --query=True --groupby_col=\"['ss_store_sk']\" --agg_col=\"['ss_sales_price']\" --where_col=\"[0]\" --where_ops=\"[['>=', '<=']]\" --where_val=\"[[2451119, 2451483]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "where_col [0]\r\n",
      "ckpts ['./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "./datasets/ss.csv\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99]\r\n",
      "encoded_bins (input) [7, 6, 7]\r\n",
      "Number of model parameters: 132333 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=20, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=237, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt 0.0019999999999997797 0\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "where_col [Column(ss_sold_date_sk, distribution_size=94)]\r\n",
      "values [36.29833333333333, 18, 653.37]\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob='ss*.pt' --dataset=ss.csv --residual --query=True --agg_col=\"['ss_sales_price']\" --where_col=\"[0]\" --where_ops=\"[['>=', '<=']]\" --where_val=\"[[2451119, 2451483]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "where_col [0, 1]\r\n",
      "ckpts ['./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "./datasets/ss.csv\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99]\r\n",
      "encoded_bins (input) [7, 6, 7]\r\n",
      "Number of model parameters: 132333 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=20, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=237, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt 0.0009999999999994458 0\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "where_col [Column(ss_sold_date_sk, distribution_size=94), Column(ss_store_sk, distribution_size=44)]\r\n",
      "values [25.64, 1, 25.64]\r\n",
      "...Done, result: results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob=ss*.pt --dataset=ss.csv --residual --query=True --agg_col=\"['ss_sales_price']\" --where_col=\"[0,1]\" --where_ops=\"[['>=', '<='],['=']]\" --where_val=\"[[2451119, 2451483], [16.0]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}