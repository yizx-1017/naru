{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "Entropy of TPCDS([Column(ss_sold_date_sk, distribution_size=94), Column(ss_store_sk, distribution_size=44), Column(ss_sales_price, distribution_size=99), Column(ss_quantity, distribution_size=60)]): 6.6439 bits\r\n",
      "<class 'pandas.core.frame.DataFrame'>\r\n",
      "RangeIndex: 100 entries, 0 to 99\r\n",
      "Data columns (total 4 columns):\r\n",
      " #   Column           Non-Null Count  Dtype  \r\n",
      "---  ------           --------------  -----  \r\n",
      " 0   ss_sold_date_sk  97 non-null     float64\r\n",
      " 1   ss_store_sk      98 non-null     float64\r\n",
      " 2   ss_sales_price   98 non-null     float64\r\n",
      " 3   ss_quantity      96 non-null     float64\r\n",
      "dtypes: float64(4)\r\n",
      "memory usage: 3.2 KB\r\n",
      "None\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99, 60]\r\n",
      "encoded_bins (input) [7, 6, 7, 6]\r\n",
      "Number of model parameters: 617596 (~= 2.4MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=26, out_features=256, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (5): MaskedLinear(in_features=256, out_features=297, bias=True)\r\n",
      "  )\r\n",
      "  (direct_io_layer): MaskedLinear(in_features=26, out_features=297, bias=True)\r\n",
      ")\r\n",
      "Applying InitWeight()\r\n",
      "Discretizing table... done, took 0.0s\r\n",
      "Epoch 0 Iter 0, train entropy gap 18.0348 bits (loss 24.679, data 6.644) 0.00000 lr\r\n",
      "epoch 0 train loss 17.1059 nats / 24.6786 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 1 Iter 0, train entropy gap 18.0347 bits (loss 24.679, data 6.644) 0.00000 lr\r\n",
      "epoch 1 train loss 17.1059 nats / 24.6786 bits\r\n",
      "time since start: 0.1 secs\r\n",
      "Epoch 2 Iter 0, train entropy gap 18.0346 bits (loss 24.678, data 6.644) 0.00000 lr\r\n",
      "epoch 2 train loss 17.1058 nats / 24.6784 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 3 Iter 0, train entropy gap 18.0344 bits (loss 24.678, data 6.644) 0.00000 lr\r\n",
      "epoch 3 train loss 17.1057 nats / 24.6783 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 4 Iter 0, train entropy gap 18.0341 bits (loss 24.678, data 6.644) 0.00000 lr\r\n",
      "epoch 4 train loss 17.1055 nats / 24.6780 bits\r\n",
      "time since start: 0.2 secs\r\n",
      "Epoch 5 Iter 0, train entropy gap 18.0338 bits (loss 24.678, data 6.644) 0.00000 lr\r\n",
      "epoch 5 train loss 17.1052 nats / 24.6777 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 6 Iter 0, train entropy gap 18.0334 bits (loss 24.677, data 6.644) 0.00000 lr\r\n",
      "epoch 6 train loss 17.1050 nats / 24.6773 bits\r\n",
      "time since start: 0.3 secs\r\n",
      "Epoch 7 Iter 0, train entropy gap 18.0330 bits (loss 24.677, data 6.644) 0.00000 lr\r\n",
      "epoch 7 train loss 17.1047 nats / 24.6768 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 8 Iter 0, train entropy gap 18.0324 bits (loss 24.676, data 6.644) 0.00000 lr\r\n",
      "epoch 8 train loss 17.1043 nats / 24.6763 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 9 Iter 0, train entropy gap 18.0318 bits (loss 24.676, data 6.644) 0.00000 lr\r\n",
      "epoch 9 train loss 17.1039 nats / 24.6757 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 10 Iter 0, train entropy gap 18.0312 bits (loss 24.675, data 6.644) 0.00000 lr\r\n",
      "epoch 10 train loss 17.1034 nats / 24.6750 bits\r\n",
      "time since start: 0.4 secs\r\n",
      "Epoch 11 Iter 0, train entropy gap 18.0305 bits (loss 24.674, data 6.644) 0.00000 lr\r\n",
      "epoch 11 train loss 17.1029 nats / 24.6743 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 12 Iter 0, train entropy gap 18.0297 bits (loss 24.674, data 6.644) 0.00000 lr\r\n",
      "epoch 12 train loss 17.1024 nats / 24.6735 bits\r\n",
      "time since start: 0.5 secs\r\n",
      "Epoch 13 Iter 0, train entropy gap 18.0288 bits (loss 24.673, data 6.644) 0.00000 lr\r\n",
      "epoch 13 train loss 17.1018 nats / 24.6727 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 14 Iter 0, train entropy gap 18.0279 bits (loss 24.672, data 6.644) 0.00000 lr\r\n",
      "epoch 14 train loss 17.1012 nats / 24.6718 bits\r\n",
      "time since start: 0.6 secs\r\n",
      "Epoch 15 Iter 0, train entropy gap 18.0269 bits (loss 24.671, data 6.644) 0.00000 lr\r\n",
      "epoch 15 train loss 17.1005 nats / 24.6708 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 16 Iter 0, train entropy gap 18.0259 bits (loss 24.670, data 6.644) 0.00000 lr\r\n",
      "epoch 16 train loss 17.0998 nats / 24.6697 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 17 Iter 0, train entropy gap 18.0248 bits (loss 24.669, data 6.644) 0.00000 lr\r\n",
      "epoch 17 train loss 17.0990 nats / 24.6686 bits\r\n",
      "time since start: 0.7 secs\r\n",
      "Epoch 18 Iter 0, train entropy gap 18.0236 bits (loss 24.667, data 6.644) 0.00000 lr\r\n",
      "epoch 18 train loss 17.0982 nats / 24.6675 bits\r\n",
      "time since start: 0.8 secs\r\n",
      "Epoch 19 Iter 0, train entropy gap 18.0224 bits (loss 24.666, data 6.644) 0.00000 lr\r\n",
      "epoch 19 train loss 17.0973 nats / 24.6662 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 20 Iter 0, train entropy gap 18.0211 bits (loss 24.665, data 6.644) 0.00001 lr\r\n",
      "epoch 20 train loss 17.0964 nats / 24.6649 bits\r\n",
      "time since start: 0.9 secs\r\n",
      "Epoch 21 Iter 0, train entropy gap 18.0197 bits (loss 24.664, data 6.644) 0.00001 lr\r\n",
      "epoch 21 train loss 17.0955 nats / 24.6635 bits\r\n",
      "time since start: 1.0 secs\r\n",
      "Epoch 22 Iter 0, train entropy gap 18.0183 bits (loss 24.662, data 6.644) 0.00001 lr\r\n",
      "epoch 22 train loss 17.0945 nats / 24.6621 bits\r\n",
      "time since start: 1.0 secs\r\n",
      "Epoch 23 Iter 0, train entropy gap 18.0168 bits (loss 24.661, data 6.644) 0.00001 lr\r\n",
      "epoch 23 train loss 17.0934 nats / 24.6606 bits\r\n",
      "time since start: 1.1 secs\r\n",
      "Epoch 24 Iter 0, train entropy gap 18.0152 bits (loss 24.659, data 6.644) 0.00001 lr\r\n",
      "epoch 24 train loss 17.0924 nats / 24.6591 bits\r\n",
      "time since start: 1.2 secs\r\n",
      "Epoch 25 Iter 0, train entropy gap 18.0136 bits (loss 24.657, data 6.644) 0.00001 lr\r\n",
      "epoch 25 train loss 17.0912 nats / 24.6574 bits\r\n",
      "time since start: 1.2 secs\r\n",
      "Epoch 26 Iter 0, train entropy gap 18.0119 bits (loss 24.656, data 6.644) 0.00001 lr\r\n",
      "epoch 26 train loss 17.0901 nats / 24.6557 bits\r\n",
      "time since start: 1.2 secs\r\n",
      "Epoch 27 Iter 0, train entropy gap 18.0101 bits (loss 24.654, data 6.644) 0.00001 lr\r\n",
      "epoch 27 train loss 17.0888 nats / 24.6540 bits\r\n",
      "time since start: 1.3 secs\r\n",
      "Epoch 28 Iter 0, train entropy gap 18.0083 bits (loss 24.652, data 6.644) 0.00001 lr\r\n",
      "epoch 28 train loss 17.0876 nats / 24.6522 bits\r\n",
      "time since start: 1.3 secs\r\n",
      "Epoch 29 Iter 0, train entropy gap 18.0064 bits (loss 24.650, data 6.644) 0.00001 lr\r\n",
      "epoch 29 train loss 17.0863 nats / 24.6503 bits\r\n",
      "time since start: 1.3 secs\r\n",
      "Epoch 30 Iter 0, train entropy gap 18.0045 bits (loss 24.648, data 6.644) 0.00001 lr\r\n",
      "epoch 30 train loss 17.0849 nats / 24.6484 bits\r\n",
      "time since start: 1.4 secs\r\n",
      "Epoch 31 Iter 0, train entropy gap 18.0025 bits (loss 24.646, data 6.644) 0.00001 lr\r\n",
      "epoch 31 train loss 17.0835 nats / 24.6464 bits\r\n",
      "time since start: 1.4 secs\r\n",
      "Epoch 32 Iter 0, train entropy gap 18.0004 bits (loss 24.644, data 6.644) 0.00001 lr\r\n",
      "epoch 32 train loss 17.0821 nats / 24.6443 bits\r\n",
      "time since start: 1.5 secs\r\n",
      "Epoch 33 Iter 0, train entropy gap 17.9983 bits (loss 24.642, data 6.644) 0.00001 lr\r\n",
      "epoch 33 train loss 17.0806 nats / 24.6422 bits\r\n",
      "time since start: 1.5 secs\r\n",
      "Epoch 34 Iter 0, train entropy gap 17.9961 bits (loss 24.640, data 6.644) 0.00001 lr\r\n",
      "epoch 34 train loss 17.0791 nats / 24.6400 bits\r\n",
      "time since start: 1.6 secs\r\n",
      "Epoch 35 Iter 0, train entropy gap 17.9939 bits (loss 24.638, data 6.644) 0.00001 lr\r\n",
      "epoch 35 train loss 17.0776 nats / 24.6377 bits\r\n",
      "time since start: 1.6 secs\r\n",
      "Epoch 36 Iter 0, train entropy gap 17.9915 bits (loss 24.635, data 6.644) 0.00001 lr\r\n",
      "epoch 36 train loss 17.0760 nats / 24.6354 bits\r\n",
      "time since start: 1.7 secs\r\n",
      "Epoch 37 Iter 0, train entropy gap 17.9892 bits (loss 24.633, data 6.644) 0.00001 lr\r\n",
      "epoch 37 train loss 17.0743 nats / 24.6330 bits\r\n",
      "time since start: 1.8 secs\r\n",
      "Epoch 38 Iter 0, train entropy gap 17.9867 bits (loss 24.631, data 6.644) 0.00001 lr\r\n",
      "epoch 38 train loss 17.0726 nats / 24.6306 bits\r\n",
      "time since start: 1.8 secs\r\n",
      "Epoch 39 Iter 0, train entropy gap 17.9842 bits (loss 24.628, data 6.644) 0.00001 lr\r\n",
      "epoch 39 train loss 17.0709 nats / 24.6281 bits\r\n",
      "time since start: 1.9 secs\r\n",
      "Epoch 40 Iter 0, train entropy gap 17.9816 bits (loss 24.625, data 6.644) 0.00001 lr\r\n",
      "epoch 40 train loss 17.0691 nats / 24.6255 bits\r\n",
      "time since start: 1.9 secs\r\n",
      "Epoch 41 Iter 0, train entropy gap 17.9790 bits (loss 24.623, data 6.644) 0.00001 lr\r\n",
      "epoch 41 train loss 17.0673 nats / 24.6229 bits\r\n",
      "time since start: 1.9 secs\r\n",
      "Epoch 42 Iter 0, train entropy gap 17.9763 bits (loss 24.620, data 6.644) 0.00001 lr\r\n",
      "epoch 42 train loss 17.0654 nats / 24.6202 bits\r\n",
      "time since start: 2.0 secs\r\n",
      "Epoch 43 Iter 0, train entropy gap 17.9736 bits (loss 24.617, data 6.644) 0.00001 lr\r\n",
      "epoch 43 train loss 17.0635 nats / 24.6174 bits\r\n",
      "time since start: 2.1 secs\r\n",
      "Epoch 44 Iter 0, train entropy gap 17.9708 bits (loss 24.615, data 6.644) 0.00001 lr\r\n",
      "epoch 44 train loss 17.0616 nats / 24.6146 bits\r\n",
      "time since start: 2.1 secs\r\n",
      "Epoch 45 Iter 0, train entropy gap 17.9679 bits (loss 24.612, data 6.644) 0.00001 lr\r\n",
      "epoch 45 train loss 17.0596 nats / 24.6117 bits\r\n",
      "time since start: 2.2 secs\r\n",
      "Epoch 46 Iter 0, train entropy gap 17.9650 bits (loss 24.609, data 6.644) 0.00001 lr\r\n",
      "epoch 46 train loss 17.0575 nats / 24.6088 bits\r\n",
      "time since start: 2.2 secs\r\n",
      "Epoch 47 Iter 0, train entropy gap 17.9620 bits (loss 24.606, data 6.644) 0.00001 lr\r\n",
      "epoch 47 train loss 17.0555 nats / 24.6058 bits\r\n",
      "time since start: 2.3 secs\r\n",
      "Epoch 48 Iter 0, train entropy gap 17.9589 bits (loss 24.603, data 6.644) 0.00001 lr\r\n",
      "epoch 48 train loss 17.0533 nats / 24.6028 bits\r\n",
      "time since start: 2.3 secs\r\n",
      "Epoch 49 Iter 0, train entropy gap 17.9558 bits (loss 24.600, data 6.644) 0.00001 lr\r\n",
      "epoch 49 train loss 17.0512 nats / 24.5997 bits\r\n",
      "time since start: 2.3 secs\r\n",
      "Epoch 50 Iter 0, train entropy gap 17.9526 bits (loss 24.596, data 6.644) 0.00001 lr\r\n",
      "epoch 50 train loss 17.0490 nats / 24.5965 bits\r\n",
      "time since start: 2.4 secs\r\n",
      "Epoch 51 Iter 0, train entropy gap 17.9494 bits (loss 24.593, data 6.644) 0.00001 lr\r\n",
      "epoch 51 train loss 17.0467 nats / 24.5933 bits\r\n",
      "time since start: 2.4 secs\r\n",
      "Epoch 52 Iter 0, train entropy gap 17.9461 bits (loss 24.590, data 6.644) 0.00001 lr\r\n",
      "epoch 52 train loss 17.0445 nats / 24.5900 bits\r\n",
      "time since start: 2.5 secs\r\n",
      "Epoch 53 Iter 0, train entropy gap 17.9428 bits (loss 24.587, data 6.644) 0.00001 lr\r\n",
      "epoch 53 train loss 17.0421 nats / 24.5866 bits\r\n",
      "time since start: 2.6 secs\r\n",
      "Epoch 54 Iter 0, train entropy gap 17.9393 bits (loss 24.583, data 6.644) 0.00001 lr\r\n",
      "epoch 54 train loss 17.0398 nats / 24.5832 bits\r\n",
      "time since start: 2.6 secs\r\n",
      "Epoch 55 Iter 0, train entropy gap 17.9359 bits (loss 24.580, data 6.644) 0.00001 lr\r\n",
      "epoch 55 train loss 17.0374 nats / 24.5797 bits\r\n",
      "time since start: 2.7 secs\r\n",
      "Epoch 56 Iter 0, train entropy gap 17.9324 bits (loss 24.576, data 6.644) 0.00001 lr\r\n",
      "epoch 56 train loss 17.0349 nats / 24.5762 bits\r\n",
      "time since start: 2.7 secs\r\n",
      "Epoch 57 Iter 0, train entropy gap 17.9288 bits (loss 24.573, data 6.644) 0.00001 lr\r\n",
      "epoch 57 train loss 17.0325 nats / 24.5726 bits\r\n",
      "time since start: 2.7 secs\r\n",
      "Epoch 58 Iter 0, train entropy gap 17.9251 bits (loss 24.569, data 6.644) 0.00001 lr\r\n",
      "epoch 58 train loss 17.0299 nats / 24.5690 bits\r\n",
      "time since start: 2.8 secs\r\n",
      "Epoch 59 Iter 0, train entropy gap 17.9214 bits (loss 24.565, data 6.644) 0.00001 lr\r\n",
      "epoch 59 train loss 17.0274 nats / 24.5653 bits\r\n",
      "time since start: 2.8 secs\r\n",
      "Epoch 60 Iter 0, train entropy gap 17.9177 bits (loss 24.562, data 6.644) 0.00002 lr\r\n",
      "epoch 60 train loss 17.0248 nats / 24.5615 bits\r\n",
      "time since start: 2.9 secs\r\n",
      "Epoch 61 Iter 0, train entropy gap 17.9139 bits (loss 24.558, data 6.644) 0.00002 lr\r\n",
      "epoch 61 train loss 17.0221 nats / 24.5577 bits\r\n",
      "time since start: 2.9 secs\r\n",
      "Epoch 62 Iter 0, train entropy gap 17.9100 bits (loss 24.554, data 6.644) 0.00002 lr\r\n",
      "epoch 62 train loss 17.0194 nats / 24.5539 bits\r\n",
      "time since start: 3.0 secs\r\n",
      "Epoch 63 Iter 0, train entropy gap 17.9061 bits (loss 24.550, data 6.644) 0.00002 lr\r\n",
      "epoch 63 train loss 17.0167 nats / 24.5499 bits\r\n",
      "time since start: 3.0 secs\r\n",
      "Epoch 64 Iter 0, train entropy gap 17.9021 bits (loss 24.546, data 6.644) 0.00002 lr\r\n",
      "epoch 64 train loss 17.0139 nats / 24.5459 bits\r\n",
      "time since start: 3.0 secs\r\n",
      "Epoch 65 Iter 0, train entropy gap 17.8980 bits (loss 24.542, data 6.644) 0.00002 lr\r\n",
      "epoch 65 train loss 17.0111 nats / 24.5419 bits\r\n",
      "time since start: 3.1 secs\r\n",
      "Epoch 66 Iter 0, train entropy gap 17.8939 bits (loss 24.538, data 6.644) 0.00002 lr\r\n",
      "epoch 66 train loss 17.0083 nats / 24.5378 bits\r\n",
      "time since start: 3.1 secs\r\n",
      "Epoch 67 Iter 0, train entropy gap 17.8898 bits (loss 24.534, data 6.644) 0.00002 lr\r\n",
      "epoch 67 train loss 17.0054 nats / 24.5336 bits\r\n",
      "time since start: 3.1 secs\r\n",
      "Epoch 68 Iter 0, train entropy gap 17.8855 bits (loss 24.529, data 6.644) 0.00002 lr\r\n",
      "epoch 68 train loss 17.0025 nats / 24.5294 bits\r\n",
      "time since start: 3.2 secs\r\n",
      "Epoch 69 Iter 0, train entropy gap 17.8812 bits (loss 24.525, data 6.644) 0.00002 lr\r\n",
      "epoch 69 train loss 16.9995 nats / 24.5251 bits\r\n",
      "time since start: 3.2 secs\r\n",
      "Epoch 70 Iter 0, train entropy gap 17.8769 bits (loss 24.521, data 6.644) 0.00002 lr\r\n",
      "epoch 70 train loss 16.9965 nats / 24.5208 bits\r\n",
      "time since start: 3.2 secs\r\n",
      "Epoch 71 Iter 0, train entropy gap 17.8725 bits (loss 24.516, data 6.644) 0.00002 lr\r\n",
      "epoch 71 train loss 16.9934 nats / 24.5164 bits\r\n",
      "time since start: 3.3 secs\r\n",
      "Epoch 72 Iter 0, train entropy gap 17.8680 bits (loss 24.512, data 6.644) 0.00002 lr\r\n",
      "epoch 72 train loss 16.9903 nats / 24.5119 bits\r\n",
      "time since start: 3.4 secs\r\n",
      "Epoch 73 Iter 0, train entropy gap 17.8635 bits (loss 24.507, data 6.644) 0.00002 lr\r\n",
      "epoch 73 train loss 16.9872 nats / 24.5074 bits\r\n",
      "time since start: 3.4 secs\r\n",
      "Epoch 74 Iter 0, train entropy gap 17.8589 bits (loss 24.503, data 6.644) 0.00002 lr\r\n",
      "epoch 74 train loss 16.9840 nats / 24.5028 bits\r\n",
      "time since start: 3.5 secs\r\n",
      "Epoch 75 Iter 0, train entropy gap 17.8543 bits (loss 24.498, data 6.644) 0.00002 lr\r\n",
      "epoch 75 train loss 16.9808 nats / 24.4982 bits\r\n",
      "time since start: 3.5 secs\r\n",
      "Epoch 76 Iter 0, train entropy gap 17.8496 bits (loss 24.493, data 6.644) 0.00002 lr\r\n",
      "epoch 76 train loss 16.9776 nats / 24.4935 bits\r\n",
      "time since start: 3.5 secs\r\n",
      "Epoch 77 Iter 0, train entropy gap 17.8449 bits (loss 24.489, data 6.644) 0.00002 lr\r\n",
      "epoch 77 train loss 16.9743 nats / 24.4887 bits\r\n",
      "time since start: 3.6 secs\r\n",
      "Epoch 78 Iter 0, train entropy gap 17.8400 bits (loss 24.484, data 6.644) 0.00002 lr\r\n",
      "epoch 78 train loss 16.9709 nats / 24.4839 bits\r\n",
      "time since start: 3.6 secs\r\n",
      "Epoch 79 Iter 0, train entropy gap 17.8352 bits (loss 24.479, data 6.644) 0.00002 lr\r\n",
      "epoch 79 train loss 16.9676 nats / 24.4790 bits\r\n",
      "time since start: 3.7 secs\r\n",
      "Epoch 80 Iter 0, train entropy gap 17.8302 bits (loss 24.474, data 6.644) 0.00002 lr\r\n",
      "epoch 80 train loss 16.9641 nats / 24.4741 bits\r\n",
      "time since start: 3.7 secs\r\n",
      "Epoch 81 Iter 0, train entropy gap 17.8252 bits (loss 24.469, data 6.644) 0.00002 lr\r\n",
      "epoch 81 train loss 16.9607 nats / 24.4691 bits\r\n",
      "time since start: 3.8 secs\r\n",
      "Epoch 82 Iter 0, train entropy gap 17.8202 bits (loss 24.464, data 6.644) 0.00002 lr\r\n",
      "epoch 82 train loss 16.9572 nats / 24.4640 bits\r\n",
      "time since start: 3.8 secs\r\n",
      "Epoch 83 Iter 0, train entropy gap 17.8151 bits (loss 24.459, data 6.644) 0.00002 lr\r\n",
      "epoch 83 train loss 16.9536 nats / 24.4589 bits\r\n",
      "time since start: 3.9 secs\r\n",
      "Epoch 84 Iter 0, train entropy gap 17.8099 bits (loss 24.454, data 6.644) 0.00002 lr\r\n",
      "epoch 84 train loss 16.9500 nats / 24.4537 bits\r\n",
      "time since start: 3.9 secs\r\n",
      "Epoch 85 Iter 0, train entropy gap 17.8046 bits (loss 24.448, data 6.644) 0.00002 lr\r\n",
      "epoch 85 train loss 16.9464 nats / 24.4485 bits\r\n",
      "time since start: 4.0 secs\r\n",
      "Epoch 86 Iter 0, train entropy gap 17.7993 bits (loss 24.443, data 6.644) 0.00002 lr\r\n",
      "epoch 86 train loss 16.9427 nats / 24.4432 bits\r\n",
      "time since start: 4.0 secs\r\n",
      "Epoch 87 Iter 0, train entropy gap 17.7940 bits (loss 24.438, data 6.644) 0.00002 lr\r\n",
      "epoch 87 train loss 16.9390 nats / 24.4378 bits\r\n",
      "time since start: 4.1 secs\r\n",
      "Epoch 88 Iter 0, train entropy gap 17.7885 bits (loss 24.432, data 6.644) 0.00002 lr\r\n",
      "epoch 88 train loss 16.9353 nats / 24.4324 bits\r\n",
      "time since start: 4.2 secs\r\n",
      "Epoch 89 Iter 0, train entropy gap 17.7831 bits (loss 24.427, data 6.644) 0.00002 lr\r\n",
      "epoch 89 train loss 16.9314 nats / 24.4269 bits\r\n",
      "time since start: 4.2 secs\r\n",
      "Epoch 90 Iter 0, train entropy gap 17.7775 bits (loss 24.421, data 6.644) 0.00002 lr\r\n",
      "epoch 90 train loss 16.9276 nats / 24.4214 bits\r\n",
      "time since start: 4.3 secs\r\n",
      "Epoch 91 Iter 0, train entropy gap 17.7719 bits (loss 24.416, data 6.644) 0.00002 lr\r\n",
      "epoch 91 train loss 16.9237 nats / 24.4158 bits\r\n",
      "time since start: 4.3 secs\r\n",
      "Epoch 92 Iter 0, train entropy gap 17.7662 bits (loss 24.410, data 6.644) 0.00002 lr\r\n",
      "epoch 92 train loss 16.9198 nats / 24.4101 bits\r\n",
      "time since start: 4.4 secs\r\n",
      "Epoch 93 Iter 0, train entropy gap 17.7605 bits (loss 24.404, data 6.644) 0.00002 lr\r\n",
      "epoch 93 train loss 16.9158 nats / 24.4044 bits\r\n",
      "time since start: 4.4 secs\r\n",
      "Epoch 94 Iter 0, train entropy gap 17.7547 bits (loss 24.399, data 6.644) 0.00002 lr\r\n",
      "epoch 94 train loss 16.9118 nats / 24.3985 bits\r\n",
      "time since start: 4.5 secs\r\n",
      "Epoch 95 Iter 0, train entropy gap 17.7488 bits (loss 24.393, data 6.644) 0.00002 lr\r\n",
      "epoch 95 train loss 16.9077 nats / 24.3927 bits\r\n",
      "time since start: 4.5 secs\r\n",
      "Epoch 96 Iter 0, train entropy gap 17.7429 bits (loss 24.387, data 6.644) 0.00002 lr\r\n",
      "epoch 96 train loss 16.9036 nats / 24.3867 bits\r\n",
      "time since start: 4.6 secs\r\n",
      "Epoch 97 Iter 0, train entropy gap 17.7369 bits (loss 24.381, data 6.644) 0.00002 lr\r\n",
      "epoch 97 train loss 16.8995 nats / 24.3808 bits\r\n",
      "time since start: 4.7 secs\r\n",
      "Epoch 98 Iter 0, train entropy gap 17.7309 bits (loss 24.375, data 6.644) 0.00002 lr\r\n",
      "epoch 98 train loss 16.8953 nats / 24.3747 bits\r\n",
      "time since start: 4.7 secs\r\n",
      "Epoch 99 Iter 0, train entropy gap 17.7247 bits (loss 24.369, data 6.644) 0.00002 lr\r\n",
      "epoch 99 train loss 16.8910 nats / 24.3686 bits\r\n",
      "time since start: 4.8 secs\r\n",
      "Training done; evaluating likelihood on full data:\r\n",
      "Epoch None Iter 0, test loss 16.8867 nats / 24.3624 bits\r\n",
      "Saved to:\r\n",
      "models/ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --epochs=100 --residual --dataset=ss_1k.csv --warmups=8000 --bs=2048 --residual --layers=5 --fc-hiddens=256 --direct-io"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "ckpts ['./models/ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99, 60]\r\n",
      "encoded_bins (input) [7, 6, 7, 6]\r\n",
      "Number of model parameters: 617596 (~= 2.4MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=26, out_features=256, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=256, out_features=256, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (5): MaskedLinear(in_features=256, out_features=297, bias=True)\r\n",
      "  )\r\n",
      "  (direct_io_layer): MaskedLinear(in_features=26, out_features=297, bias=True)\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "./models/ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt 17.717999999999996 0\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "where_col [Column(ss_sold_date_sk, distribution_size=94)]\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob='ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt' --dataset=ss.csv --residual --layers=5 --fc-hiddens=256 --direct-io --query=True --agg_col=\"ss_sales_price\" --where_col=\"['ss_sold_date_sk']\" --where_ops=\"[['>=', '<=']]\" --where_val=\"[[2451119, 2451483]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "ckpts ['./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt', './models/ss.csv-2.4MB-model24.362-data6.644-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99, 60]\r\n",
      "encoded_bins (input) [7, 6, 7, 6]\r\n",
      "Number of model parameters: 140841 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=26, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=297, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-0.5MB-model6.646-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"eval_model.py\", line 686, in <module>\r\n",
      "    Main()\r\n",
      "  File \"eval_model.py\", line 605, in Main\r\n",
      "    model.load_state_dict(torch.load(s))\r\n",
      "  File \"/Users/eve/opt/anaconda3/envs/work/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1497, in load_state_dict\r\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n",
      "RuntimeError: Error(s) in loading state_dict for MADE:\r\n",
      "\tsize mismatch for net.0.weight: copying a param with shape torch.Size([128, 20]) from checkpoint, the shape in current model is torch.Size([128, 26]).\r\n",
      "\tsize mismatch for net.0.mask: copying a param with shape torch.Size([128, 20]) from checkpoint, the shape in current model is torch.Size([128, 26]).\r\n",
      "\tsize mismatch for net.4.weight: copying a param with shape torch.Size([237, 128]) from checkpoint, the shape in current model is torch.Size([297, 128]).\r\n",
      "\tsize mismatch for net.4.bias: copying a param with shape torch.Size([237]) from checkpoint, the shape in current model is torch.Size([297]).\r\n",
      "\tsize mismatch for net.4.mask: copying a param with shape torch.Size([237, 128]) from checkpoint, the shape in current model is torch.Size([297, 128]).\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob='ss*.pt' --dataset=ss.csv --residual --query=True --agg_col=\"['ss_sales_price']\" --where_col=\"[0]\" --where_ops=\"[['>=', '<=']]\" --where_val=\"[[2451119, 2451483]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\r\n",
      "where_col [0, 1]\r\n",
      "ckpts ['./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt']\r\n",
      "./datasets/ss.csv\r\n",
      "Loading csv... done, took 0.0s\r\n",
      "Parsing... done, took 0.0s\r\n",
      "fixed_ordering None seed 0 natural_ordering True\r\n",
      "encoded_bins (output) [94, 44, 99]\r\n",
      "encoded_bins (input) [7, 6, 7]\r\n",
      "Number of model parameters: 132333 (~= 0.5MB)\r\n",
      "MADE(\r\n",
      "  (net): Sequential(\r\n",
      "    (0): MaskedLinear(in_features=20, out_features=128, bias=True)\r\n",
      "    (1): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (2): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (3): MaskedResidualBlock(\r\n",
      "      (layers): ModuleList(\r\n",
      "        (0): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "        (1): MaskedLinear(in_features=128, out_features=128, bias=True)\r\n",
      "      )\r\n",
      "      (activation): ReLU()\r\n",
      "    )\r\n",
      "    (4): MaskedLinear(in_features=128, out_features=237, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "Loading ckpt: ./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt\r\n",
      "./models/ss.csv-0.5MB-model6.645-data6.644-made-resmade-hidden128_128_128_128-emb32-nodirectIo-binaryInone_hotOut-inputNoEmbIfLeq-100epochs-seed0.pt 0.0009999999999994458 0\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "Setting masked_weight in MADE, do not retrain!\r\n",
      "where_col [Column(ss_sold_date_sk, distribution_size=94), Column(ss_store_sk, distribution_size=44)]\r\n",
      "values [25.64, 1, 25.64]\r\n",
      "...Done, result: results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_model.py --glob=ss*.pt --dataset=ss.csv --residual --query=True --agg_col=\"['ss_sales_price']\" --where_col=\"[0,1]\" --where_ops=\"[['>=', '<='],['=']]\" --where_val=\"[[2451119, 2451483], [16.0]]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}